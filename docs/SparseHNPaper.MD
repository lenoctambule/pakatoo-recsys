# Sparsely Activated Hopfield Network (SpAHN)

## Abstract

Most approaches in the design of deep learning architecture are layered and top-down, however Hopfield Networks are one of the rare cases, maybe the only one, where the design is bottom-up by taking inspiration from statistical physics. More importantly the system also seems to be functionning from the bottom-up as it does not require layer to layer algorithms such as backpropagation. This property is interesting computationally because it might allow us to design them such that they are sparsely activated which could  help optimize training and inference times for CPUs aswell as saving energy but could also allow for online training. That is only if we are able to make these networks to be sparsely activated during both training and inference.

This paper introduces Sparsely Activated Hopfield Neural Network such that it is more computationally compatible with non-neuromorphic or specialized hardware.

## Model

## Results

## Discussion